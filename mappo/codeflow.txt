Implementation of MAPPO algorithm refered "https://github.com/marlbenchmark/on-policy"

<MAPPO>
* 이 repo는 environment가 여러 개다. 그래서 각자 env마다 돌아가는 코드가 다른다.
  (소개된 scenario: SMAC, Hanabi, Multiagent Particle-World Environemtns, Google Research Football(GRF))
* 여기서는 SMAC 시뮬레이터 기준으로 설명하겠다. "football_runner.py"파일 참고
* patrial observation 환경과 fully sharing observation 환경이 있는데,
우리가 주목해야할 환경은 fully sharing observarion 환경이다.

기존과 동일하게 일정 Step동안 buffer에 저장 후, update 진행

* 기존 PPO 알고리즘과 같이 MAPPO 또한 On-policy 알고리즘이다.
   (training script path: train_mpe.sh)




----------- football_runner.py -------------

def run -> main environment running code

1. episode동안 학습을 진행한다.
2. self.collect(step)을 통해 현재 status에서 action을 뽑아낸다.
3. epsiode에서 일정 step 동안은 replay buffer에 저장만 한다. 일정 step이 지나면 network를 update한다.
	(self.train())
4. training된 model을 save한다. (self.save())
5. eval_interval마다 학습된 model을 evaluation한다. (얼마나 학습이 되었는지 실제로 exam)
 (self.eval(total_num_steps))

def collect(self, step) -> 여기서 self.trainer.policy.get_actions 함수를 호출
   (각자 agent가 가지고 있는 policy에 따라서 값을 뱉어낸다)

def warmup -> initialization part

def render -> draw current update status in graphic motion

