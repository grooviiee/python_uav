[WORLD_STEP] MBS ACTION: type (<class 'numpy.ndarray'>) len (100) action ([ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True])
[WORLD_STEP] UAV ACTION: type (<class 'list'>) len (32) action ([array([10,  0,  0,  0], dtype=int64), array([9, 0, 0, 0], dtype=int64), array([9, 0, 0, 0], dtype=int64), array([19,  0,  0,  0], dtype=int64), array([18,  0,  0,  0], dtype=int64), array([10,  0,  0,  0], dtype=int64), array([16,  0,  0,  0], dtype=int64), array([17,  0,  0,  0], dtype=int64), array([7, 0, 0, 0], dtype=int64), array([28,  0,  0,  0], dtype=int64), array([23,  0,  0,  0], dtype=int64), array([13,  0,  0,  0], dtype=int64), array([20,  0,  0,  0], dtype=int64), array([11,  0,  0,  0], dtype=int64), array([8, 0, 0, 0], dtype=int64), array([20,  0,  0,  0], dtype=int64), array([17,  0,  0,  0], dtype=int64), array([17,  0,  0,  0], dtype=int64), array([4, 0, 0, 0], dtype=int64), array([16,  0,  0,  0], dtype=int64), array([16,  0,  0,  0], dtype=int64), array([25,  0,  0,  0], dtype=int64), array([20,  0,  0,  0], dtype=int64), array([22,  0,  0,  0], dtype=int64), array([12,  0,  0,  0], dtype=int64), array([18,  0,  0,  0], dtype=int64), array([29,  0,  0,  0], dtype=int64), array([21,  0,  0,  0], dtype=int64), array([16,  0,  0,  0], dtype=int64), array([0, 0, 0, 0], dtype=int64), array([28,  0,  0,  0], dtype=int64), array([9, 0, 0, 0], dtype=int64)])
[uav_apply_cache] agent_id (<envs.core.Agent object at 0x000001F28F33D8B0>) action_cache (10) type (<class 'numpy.int64'>)
[uav_apply_power] <envs.core.Agent object at 0x000001F28F33D8B0>, 0
[WORLD_STEP] UAV ACTION: type (<class 'list'>) len (32) action ([array([22,  0,  0,  0], dtype=int64), array([9, 0, 0, 0], dtype=int64), array([15,  0,  0,  0], dtype=int64), array([21,  0,  0,  0], dtype=int64), array([4, 0, 0, 0], dtype=int64), array([20,  0,  0,  0], dtype=int64), array([25,  0,  0,  0], dtype=int64), array([21,  0,  0,  0], dtype=int64), array([1, 0, 0, 0], dtype=int64), array([15,  0,  0,  0], dtype=int64), array([7, 0, 0, 0], dtype=int64), array([6, 0, 0, 0], dtype=int64), array([0, 0, 0, 0], dtype=int64), array([22,  0,  0,  0], dtype=int64), array([1, 0, 0, 0], dtype=int64), array([29,  0,  0,  0], dtype=int64), array([6, 0, 0, 0], dtype=int64), array([9, 0, 0, 0], dtype=int64), array([10,  0,  0,  0], dtype=int64), array([14,  0,  0,  0], dtype=int64), array([23,  0,  0,  0], dtype=int64), array([4, 0, 0, 0], dtype=int64), array([11,  0,  0,  0], dtype=int64), array([4, 0, 0, 0], dtype=int64), array([29,  0,  0,  0], dtype=int64), array([5, 0, 0, 0], dtype=int64), array([16,  0,  0,  0], dtype=int64), array([10,  0,  0,  0], dtype=int64), array([13,  0,  0,  0], dtype=int64), array([1, 0, 0, 0], dtype=int64), array([0, 0, 0, 0], dtype=int64), array([13,  0,  0,  0], dtype=int64)])
[uav_apply_cache] agent_id (<envs.core.Agent object at 0x000001F28F33DEB0>) action_cache (22) type (<class 'numpy.int64'>)
[uav_apply_power] <envs.core.Agent object at 0x000001F28F33DEB0>, 0
[WORLD_STEP] UAV ACTION: type (<class 'list'>) len (32) action ([array([11,  0,  0,  0], dtype=int64), array([21,  0,  0,  0], dtype=int64), array([3, 0, 0, 0], dtype=int64), array([2, 0, 0, 0], dtype=int64), array([3, 0, 0, 0], dtype=int64), array([11,  0,  0,  0], dtype=int64), array([1, 0, 0, 0], dtype=int64), array([18,  0,  0,  0], dtype=int64), array([5, 0, 0, 0], dtype=int64), array([24,  0,  0,  0], dtype=int64), array([12,  0,  0,  0], dtype=int64), array([12,  0,  0,  0], dtype=int64), array([3, 0, 0, 0], dtype=int64), array([28,  0,  0,  0], dtype=int64), array([16,  0,  0,  0], dtype=int64), array([14,  0,  0,  0], dtype=int64), array([22,  0,  0,  0], dtype=int64), array([21,  0, 
 0,  0], dtype=int64), array([7, 0, 0, 0], dtype=int64), array([22,  0,  0,  0], dtype=int64), array([8, 0, 0, 0], dtype=int64), array([27,  0,  0,  0], dtype=int64), array([24,  0,  0,  0], dtype=int64), array([27,  0,  0,  0], dtype=int64), array([7, 0, 0, 0], dtype=int64), array([6, 0, 0, 0], dtype=int64), array([7, 0, 0, 0], dtype=int64), array([18,  0,  0,  0], dtype=int64), array([25,  0,  0,  0], dtype=int64), array([24,  0,  0,  0], dtype=int64), array([8, 0, 0, 0], dtype=int64), array([27,  0,  0,  0], dtype=int64)])
[uav_apply_cache] agent_id (<envs.core.Agent object at 0x000001F28F33D6D0>) action_cache (11) type (<class 'numpy.int64'>)
[uav_apply_power] <envs.core.Agent object at 0x000001F28F33D6D0>, 0
[WORLD_STEP] UAV ACTION: type (<class 'list'>) len (32) action ([array([9, 0, 0, 0], dtype=int64), array([7, 0, 0, 0], dtype=int64), array([11,  0,  0,  0], dtype=int64), array([18,  0,  0,  0], dtype=int64), array([15,  0,  0,  0], dtype=int64), array([25,  0,  0,  0], dtype=int64), array([4, 0, 0, 0], dtype=int64), array([28,  0,  0,  0], dtype=int64), array([11,  0,  0,  0], dtype=int64), array([2, 0, 0, 0], dtype=int64), array([27,  0,  0,  0], dtype=int64), array([16,  0,  0,  0], dtype=int64), array([0, 0, 0, 0], dtype=int64), array([18,  0,  0,  0], dtype=int64), array([11,  0,  0,  0], dtype=int64), array([17,  0,  0,  0], dtype=int64), array([1, 0, 0, 0], dtype=int64), array([25,  0, 
 0,  0], dtype=int64), array([8, 0, 0, 0], dtype=int64), array([28,  0,  0,  0], dtype=int64), array([5, 0, 0, 0], dtype=int64), array([14,  0,  0,  0], dtype=int64), array([10,  0,  0,  0], dtype=int64), array([6, 0, 0, 0], dtype=int64), array([29,  0,  0,  0], dtype=int64), array([9, 0, 0, 0], dtype=int64), array([1, 0, 0, 0], dtype=int64), array([25,  0,  0,  0], dtype=int64), array([5, 0, 0, 0], dtype=int64), array([24,  0,  0,  0], dtype=int64), array([8, 0, 
0, 0], dtype=int64), array([24,  0,  0,  0], dtype=int64)])
[uav_apply_cache] agent_id (<envs.core.Agent object at 0x000001F28F3233A0>) action_cache (9) type (<class 'numpy.int64'>)
[uav_apply_power] <envs.core.Agent object at 0x000001F28F3233A0>, 0
[MBS_STATE] id(0)
[MBS_STATE] id(0)
[UAV_STATE] id(1), (x,y): (1709,1373)
[UAV_STATE] id(1), (x,y): (1709,1373)
[UAV_STATE] id(2), (x,y): (1504,1799)
[UAV_STATE] id(2), (x,y): (1504,1799)
[UAV_STATE] id(3), (x,y): (551,1567)
[UAV_STATE] id(3), (x,y): (551,1567)
[UAV_STATE] id(4), (x,y): (1428,669)
[UAV_STATE] id(4), (x,y): (1428,669)
[USER_STATE] id(0), (x,y): (939, 1676), file_request: (2)
[USER_STATE] id(0), (x,y): (939, 1676), file_request: (2)
[USER_STATE] id(1), (x,y): (1726, 1551), file_request: (3)
[USER_STATE] id(1), (x,y): (1726, 1551), file_request: (3)
[USER_STATE] id(2), (x,y): (448, 1560), file_request: (1)
[USER_STATE] id(2), (x,y): (448, 1560), file_request: (1)
[USER_STATE] id(3), (x,y): (973, 779), file_request: (1)
[USER_STATE] id(3), (x,y): (973, 779), file_request: (1)
[USER_STATE] id(4), (x,y): (339, 1514), file_request: (3)
[USER_STATE] id(4), (x,y): (339, 1514), file_request: (3)
[USER_STATE] id(5), (x,y): (1388, 103), file_request: (1)
[USER_STATE] id(5), (x,y): (1388, 103), file_request: (1)
[USER_STATE] id(6), (x,y): (1711, 1059), file_request: (1)
[USER_STATE] id(6), (x,y): (1711, 1059), file_request: (1)
[USER_STATE] id(7), (x,y): (544, 118), file_request: (1)
[USER_STATE] id(7), (x,y): (544, 118), file_request: (1)
[USER_STATE] id(8), (x,y): (675, 1266), file_request: (1)
[USER_STATE] id(8), (x,y): (675, 1266), file_request: (1)
[USER_STATE] id(9), (x,y): (424, 100), file_request: (182)
[USER_STATE] id(9), (x,y): (424, 100), file_request: (182)
[USER_STATE] id(10), (x,y): (1106, 386), file_request: (19)
[USER_STATE] id(10), (x,y): (1106, 386), file_request: (19)
[USER_STATE] id(11), (x,y): (870, 106), file_request: (1)
[USER_STATE] id(11), (x,y): (870, 106), file_request: (1)
[USER_STATE] id(12), (x,y): (1076, 810), file_request: (2)
[USER_STATE] id(12), (x,y): (1076, 810), file_request: (2)
[USER_STATE] id(13), (x,y): (1560, 659), file_request: (1)
[USER_STATE] id(13), (x,y): (1560, 659), file_request: (1)
[USER_STATE] id(14), (x,y): (252, 364), file_request: (25)
[USER_STATE] id(14), (x,y): (252, 364), file_request: (25)
[USER_STATE] id(15), (x,y): (1150, 976), file_request: (2)
[USER_STATE] id(15), (x,y): (1150, 976), file_request: (2)
[USER_STATE] id(16), (x,y): (1035, 1756), file_request: (5)
[USER_STATE] id(16), (x,y): (1035, 1756), file_request: (5)
[USER_STATE] id(17), (x,y): (232, 1045), file_request: (1)
[USER_STATE] id(17), (x,y): (232, 1045), file_request: (1)
[USER_STATE] id(18), (x,y): (241, 788), file_request: (1)
[USER_STATE] id(18), (x,y): (241, 788), file_request: (1)
[USER_STATE] id(19), (x,y): (1755, 1705), file_request: (1)
[USER_STATE] id(19), (x,y): (1755, 1705), file_request: (1)
is_done: True, self.current_step: 50, self.world_length: 50
is_done: True, self.current_step: 50, self.world_length: 50
is_done: True, self.current_step: 50, self.world_length: 50
is_done: True, self.current_step: 50, self.world_length: 50
is_done: True, self.current_step: 50, self.world_length: 50
[ENV_STEP] get reward_n: [[-9.170334451352131e+20], [nan], [nan], [nan], [nan]], self.shared_reward: True, reward: nan
[RUNNER] Get rewards: [[nan], [nan], [nan], [nan], [nan]]
[RUNNER_INSERT] MAKE_SHARE_OBS: idx: 0, len(obs[idx]): 3, len(share_obs): 1
[RUNNER_INSERT] MAKE_SHARE_OBS: idx: 1, len(obs[idx]): 3, len(share_obs): 2
[RUNNER_INSERT] MAKE_SHARE_OBS: idx: 2, len(obs[idx]): 3, len(share_obs): 3
[RUNNER_INSERT] MAKE_SHARE_OBS: idx: 3, len(obs[idx]): 3, len(share_obs): 4
[RUNNER_INSERT] MAKE_SHARE_OBS: idx: 4, len(obs[idx]): 3, len(share_obs): 5
[RUNNER_INSERT] SHARE_OBS len(share_obs): 5
[INSERT_BUFFER] obs type (<class 'list'>) len_obs: [173, 1110, 1709.0, 1373.0, 1504.0, 1799.0, 551.0, 1567.0, 1428.0, 669.0, 939, 1676, 1726, 1551, 448, 1560, 973, 779, 339, 1514, 1388, 103, 1711, 1059, 544, 118, 675, 1266, 424, 100, 
1106, 386, 870, 106, 1076, 810, 1560, 659, 252, 364, 1150, 976, 1035, 1756, 232, 1045, 241, 788, 1755, 1705](50) len_buffer.obs: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0.]]((1, 50))
[INSERT_BUFFER] actions type (<class 'numpy.ndarray'>) shape_actions ((32, 100)) shape_buffer.actions ((1, 100))
[INSERT_BUFFER] action_log_probs type (<class 'numpy.ndarray'>) shape_action_log_probs: (32, 1), shape_buffer_action_log_probs: (1, 100)
[INSERT_BUFFER] obs type (<class 'list'>) len_obs: [1.709e+03 1.373e+03 9.390e+02 1.676e+03 1.726e+03 1.551e+03 4.480e+02
 1.560e+03 9.730e+02 7.790e+02 3.390e+02 1.514e+03 1.388e+03 1.030e+02
 1.711e+03 1.059e+03 5.440e+02 1.180e+02 6.750e+02 1.266e+03 4.240e+02
 1.000e+02 1.106e+03 3.860e+02 8.700e+02 1.060e+02 1.076e+03 8.100e+02
 1.560e+03 6.590e+02 2.520e+02 3.640e+02 1.150e+03 9.760e+02 1.035e+03
 1.756e+03 2.320e+02 1.045e+03 2.410e+02 7.880e+02 1.755e+03 1.705e+03
 2.000e+00 3.000e+00 1.000e+00 1.000e+00 3.000e+00 1.000e+00 1.000e+00
 1.000e+00 1.000e+00 1.820e+02 1.900e+01 1.000e+00 2.000e+00 1.000e+00
 2.500e+01 2.000e+00 5.000e+00 1.000e+00 1.000e+00 1.000e+00](62) len_buffer.obs: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]((1, 62))
[INSERT_BUFFER] actions type (<class 'numpy.ndarray'>) shape_actions ((32, 4)) shape_buffer.actions ((1, 4))
[INSERT_BUFFER] action_log_probs type (<class 'numpy.ndarray'>) shape_action_log_probs: (32, 4), shape_buffer_action_log_probs: (1, 4)
[INSERT_BUFFER] obs type (<class 'list'>) len_obs: [1.504e+03 1.799e+03 9.390e+02 1.676e+03 1.726e+03 1.551e+03 4.480e+02
 1.560e+03 9.730e+02 7.790e+02 3.390e+02 1.514e+03 1.388e+03 1.030e+02
 1.711e+03 1.059e+03 5.440e+02 1.180e+02 6.750e+02 1.266e+03 4.240e+02
 1.000e+02 1.106e+03 3.860e+02 8.700e+02 1.060e+02 1.076e+03 8.100e+02
 1.560e+03 6.590e+02 2.520e+02 3.640e+02 1.150e+03 9.760e+02 1.035e+03
 1.756e+03 2.320e+02 1.045e+03 2.410e+02 7.880e+02 1.755e+03 1.705e+03
 2.000e+00 3.000e+00 1.000e+00 1.000e+00 3.000e+00 1.000e+00 1.000e+00
 1.000e+00 1.000e+00 1.820e+02 1.900e+01 1.000e+00 2.000e+00 1.000e+00
 2.500e+01 2.000e+00 5.000e+00 1.000e+00 1.000e+00 1.000e+00](62) len_buffer.obs: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]((1, 62))
[INSERT_BUFFER] actions type (<class 'numpy.ndarray'>) shape_actions ((32, 4)) shape_buffer.actions ((1, 4))
[INSERT_BUFFER] action_log_probs type (<class 'numpy.ndarray'>) shape_action_log_probs: (32, 4), shape_buffer_action_log_probs: (1, 4)
[INSERT_BUFFER] obs type (<class 'list'>) len_obs: [5.510e+02 1.567e+03 9.390e+02 1.676e+03 1.726e+03 1.551e+03 4.480e+02
 1.560e+03 9.730e+02 7.790e+02 3.390e+02 1.514e+03 1.388e+03 1.030e+02
 1.711e+03 1.059e+03 5.440e+02 1.180e+02 6.750e+02 1.266e+03 4.240e+02
 1.000e+02 1.106e+03 3.860e+02 8.700e+02 1.060e+02 1.076e+03 8.100e+02
 1.560e+03 6.590e+02 2.520e+02 3.640e+02 1.150e+03 9.760e+02 1.035e+03
 1.756e+03 2.320e+02 1.045e+03 2.410e+02 7.880e+02 1.755e+03 1.705e+03
 2.000e+00 3.000e+00 1.000e+00 1.000e+00 3.000e+00 1.000e+00 1.000e+00
 1.000e+00 1.000e+00 1.820e+02 1.900e+01 1.000e+00 2.000e+00 1.000e+00
 2.500e+01 2.000e+00 5.000e+00 1.000e+00 1.000e+00 1.000e+00](62) len_buffer.obs: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]((1, 62))
[INSERT_BUFFER] actions type (<class 'numpy.ndarray'>) shape_actions ((32, 4)) shape_buffer.actions ((1, 4))
[INSERT_BUFFER] action_log_probs type (<class 'numpy.ndarray'>) shape_action_log_probs: (32, 4), shape_buffer_action_log_probs: (1, 4)
[INSERT_BUFFER] obs type (<class 'list'>) len_obs: [1.428e+03 6.690e+02 9.390e+02 1.676e+03 1.726e+03 1.551e+03 4.480e+02
 1.560e+03 9.730e+02 7.790e+02 3.390e+02 1.514e+03 1.388e+03 1.030e+02
 1.711e+03 1.059e+03 5.440e+02 1.180e+02 6.750e+02 1.266e+03 4.240e+02
 1.000e+02 1.106e+03 3.860e+02 8.700e+02 1.060e+02 1.076e+03 8.100e+02
 1.560e+03 6.590e+02 2.520e+02 3.640e+02 1.150e+03 9.760e+02 1.035e+03
 1.756e+03 2.320e+02 1.045e+03 2.410e+02 7.880e+02 1.755e+03 1.705e+03
 2.000e+00 3.000e+00 1.000e+00 1.000e+00 3.000e+00 1.000e+00 1.000e+00
 1.000e+00 1.000e+00 1.820e+02 1.900e+01 1.000e+00 2.000e+00 1.000e+00
 2.500e+01 2.000e+00 5.000e+00 1.000e+00 1.000e+00 1.000e+00](62) len_buffer.obs: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]((1, 62))
[INSERT_BUFFER] actions type (<class 'numpy.ndarray'>) shape_actions ((32, 4)) shape_buffer.actions ((1, 4))
[INSERT_BUFFER] action_log_probs type (<class 'numpy.ndarray'>) shape_action_log_probs: (32, 4), shape_buffer_action_log_probs: (1, 4)
[RUNNER_REWARD] individual_reward: [[-9.170334451352131e+20], [nan], [nan], [nan], [nan]]
[RUNNER_REWARD] tatal_reward: nan
[RUNNER_REWARD] tatal_reward: nan
[RUNNER] Compute GAE
[RUNNER_BUFFER_INSERT] agent_id:0
share_obs:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0.]]
rnn_states_critic:[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]], masks: [[1.]]
[CRITIC_FORWARD] cent_obs.shape: (2, 5, 5), _use_naive_recurrent_policy:False, _use_recurrent_policy:False
[CNN_FORWARD]: (forward) input x: torch.Size([2, 5, 5])
[CNN_FORWARD]: (forward_after_self.cnn(x)) returned x: torch.Size([32, 64])
[COMPUTE_GAE] (Compute_returns) step: 50, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 49, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 48, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 47, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 46, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 45, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 44, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 43, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 42, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 41, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 40, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 39, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 38, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 37, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 36, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 35, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 34, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 33, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 32, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 31, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 30, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 29, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 28, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 27, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 26, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 25, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 24, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 23, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 22, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 21, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 20, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 19, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 18, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 17, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 16, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 15, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 14, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 13, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 12, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 11, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 10, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 9, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 8, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 7, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 6, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 5, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 4, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 3, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 2, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 1, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 0, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[RUNNER_BUFFER_INSERT] agent_id:1
share_obs:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
rnn_states_critic:[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]], masks: [[1.]]
[CRITIC_FORWARD] cent_obs.shape: (1, 2, 31), _use_naive_recurrent_policy:False, _use_recurrent_policy:False
[CNN_FORWARD]: (forward) input x: torch.Size([1, 2, 31])
[CNN_FORWARD]: (forward_after_self.cnn(x)) returned x: torch.Size([32, 64])
[COMPUTE_GAE] (Compute_returns) step: 50, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 49, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 48, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 47, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 46, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 45, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 44, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 43, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 42, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 41, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 40, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 39, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 38, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 37, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 36, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 35, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 34, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 33, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 32, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 31, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 30, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 29, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 28, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 27, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 26, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 25, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 24, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 23, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 22, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 21, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 20, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 19, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 18, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 17, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 16, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 15, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 14, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 13, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 12, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 11, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 10, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 9, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 8, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 7, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 6, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 5, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 4, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 3, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 2, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 1, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 0, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[RUNNER_BUFFER_INSERT] agent_id:2
share_obs:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
rnn_states_critic:[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]], masks: [[1.]]
[CRITIC_FORWARD] cent_obs.shape: (1, 2, 31), _use_naive_recurrent_policy:False, _use_recurrent_policy:False
[CNN_FORWARD]: (forward) input x: torch.Size([1, 2, 31])
[CNN_FORWARD]: (forward_after_self.cnn(x)) returned x: torch.Size([32, 64])
[COMPUTE_GAE] (Compute_returns) step: 50, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 49, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 48, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 47, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 46, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 45, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 44, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 43, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 42, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 41, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 40, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 39, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 38, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 37, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 36, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 35, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 34, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 33, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 32, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 31, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 30, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 29, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 28, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 27, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 26, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 25, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 24, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 23, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 22, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 21, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 20, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 19, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 18, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 17, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 16, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 15, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 14, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 13, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 12, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 11, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 10, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 9, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 8, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 7, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 6, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 5, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 4, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 3, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 2, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 1, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 0, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[RUNNER_BUFFER_INSERT] agent_id:3
share_obs:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
rnn_states_critic:[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]], masks: [[1.]]
[CRITIC_FORWARD] cent_obs.shape: (1, 2, 31), _use_naive_recurrent_policy:False, _use_recurrent_policy:False
[CNN_FORWARD]: (forward) input x: torch.Size([1, 2, 31])
[CNN_FORWARD]: (forward_after_self.cnn(x)) returned x: torch.Size([32, 64])
[COMPUTE_GAE] (Compute_returns) step: 50, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 49, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 48, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 47, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 46, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 45, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 44, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 43, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 42, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 41, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 40, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 39, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 38, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 37, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 36, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 35, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 34, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 33, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 32, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 31, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 30, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 29, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 28, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 27, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 26, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 25, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 24, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 23, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 22, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 21, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 20, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 19, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 18, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 17, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 16, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 15, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 14, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 13, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 12, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 11, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 10, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 9, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 8, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 7, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 6, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 5, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 4, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 3, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 2, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 1, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 0, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[RUNNER_BUFFER_INSERT] agent_id:4
share_obs:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
rnn_states_critic:[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]], masks: [[1.]]
[CRITIC_FORWARD] cent_obs.shape: (1, 2, 31), _use_naive_recurrent_policy:False, _use_recurrent_policy:False
[CNN_FORWARD]: (forward) input x: torch.Size([1, 2, 31])
[CNN_FORWARD]: (forward_after_self.cnn(x)) returned x: torch.Size([32, 64])
[COMPUTE_GAE] (Compute_returns) step: 50, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 49, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 48, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 47, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 46, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 45, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 44, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 43, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 42, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 41, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 40, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 39, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 38, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 37, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 36, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 35, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 34, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 33, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 32, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 31, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 30, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 29, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 28, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 27, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 26, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 25, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 24, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 23, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 22, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 21, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 20, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 19, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 18, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 17, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 16, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 15, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 14, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 13, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 12, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 11, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 10, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 9, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 8, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 7, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 6, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 5, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 4, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 3, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 2, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 1, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[COMPUTE_GAE] (Compute_returns) step: 0, self.rewards.shape[0]: 51, masks: 52, rewards: 51
[RUNNER] TRAIN
[TRAIN] ppo_epoch: 15
[TRAIN] data_generator <generator object SeparatedReplayBuffer.feed_forward_generator at 0x000001F2991A5EB0>, num_mini_batch: 1
[FEED_FORWARD_GEN] batch_size: 51 = episode_length: 51 * n_rollout_threads: 1 || num_mini_batch: 1
[TRAIN] ppo_update. idx (0) is_uav (False)
share_obs_batch ((51, 50)), obs_batch ((51, 50))
[(R_ACTOR)EVALUATE_ACTION] <input> obs: tensor([[[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]],

         [[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]],

         [[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]],

         [[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]],

         [[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]],

         [[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]],

         [[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]]]], device='cuda:0')
[CNN_FORWARD]: (forward) input x: torch.Size([51, 2, 5, 5])
Traceback (most recent call last):
  File "F:\Code\python_uav\main.py", line 155, in <module>
    main(arglist)
  File "F:\Code\python_uav\main.py", line 139, in main
    runner.run()
  File "F:\Code\python_uav\runner\singleBS_runner.py", line 159, in run
    train_infos = self.train()
  File "F:\Code\python_uav\runner\base_runner.py", line 60, in train
    train_info = self.trainer[agent_id].train(is_uav, self.buffer[agent_id])
  File "F:\Code\python_uav\algorithms\mappo.py", line 77, in train
    value_loss, critic_grad_norm, policy_loss, dist_entropy, actor_grad_norm, imp_weights = self.ppo_update(is_uav, sample, update_actor)
  File "F:\Code\python_uav\algorithms\mappo.py", line 119, in ppo_update
    values, action_log_probs, dist_entropy = self.policy.evaluate_actions(is_uav, share_obs_batch,
  File "F:\Code\python_uav\algorithms\algorithm\mappoPolicy.py", line 102, in evaluate_actions
    action_log_probs, dist_entropy = self.actor.evaluate_actions(cent_obs,
  File "F:\Code\python_uav\algorithms\algorithm\r_actor.py", line 117, in evaluate_actions
    actor_features = self.base(obs)
  File "C:\Users\June\.conda\envs\test_env\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "F:\Code\python_uav\algorithms\utils\cnn.py", line 117, in forward
    x = self.cnn(x)
  File "C:\Users\June\.conda\envs\test_env\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "F:\Code\python_uav\algorithms\utils\cnn.py", line 94, in forward
    x = self.cnn(x)
  File "C:\Users\June\.conda\envs\test_env\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\June\.conda\envs\test_env\lib\site-packages\torch\nn\modules\container.py", line 139, in forward
    input = module(input)
  File "C:\Users\June\.conda\envs\test_env\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\June\.conda\envs\test_env\lib\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (51x128 and 4x64)
wandb: Waiting for W&B process to finish... (failed 1). Press Ctrl-C to abort syncing.
wandb:  View run mappo_check_seed1 at: https://wandb.ai/grooviiee2/python_uav/runs/a38yjz2i
wandb:  View job at https://wandb.ai/grooviiee2/python_uav/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg1NTA4MzQz/version_details/v30
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: F:\Code\results\uavnet\scenario_ref\mappo\check\wandb\run-20231010_235547-a38yjz2i\logs





