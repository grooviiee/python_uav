<MAPPO> - 2022년에 나온 "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games" 논문 검증을 위한 시뮬레이터
타 알고리즘과 동일하게 CTDE 방식을 사용해서 학습한다.

* 이 repo는 env가 여러개라서 각자 env마다 돌아가는 코드가 다른다.
* 여기서는 SMAC 시뮬레이터 기준으로 설명 ("smac_runner.py"파일에서 실행)

기존과 동일하게 일정 Step동안 buffer에 저장 후, update 진행

on-policy/algorithms
 -> 실질적으로 RL Training을 실행하는 폴더
 -- r_mappo/r_actor_critic.py : 
    - Actor와 critic 클래스가 각각 존재.
	-- Actor: Outputs actions given observations.
	-- Critic: Outputs value function predictions given centralized input (MAPPO) or local observations (IPPO).
 -- r_mappo/rMAPPOPolicy.py : Wraps actor and critic networks to compute actions and value function predictions.
 - __init__.py : 
 - r_mappo.py : Trainer class for MAPPO to update policies.
 
 
on-policy/envs
 -> 여러가지 환경에서의 강화학습을 제공한다. 여기는 그 환경에 대한 코드
 - core.py: 시뮬레이션의 모든 Entity 정보를 다룬다. agent와 user의 행동에 따른 state를 수정
  
 
on-policy/runner
 - seperated buffer와 shared buffer 환경을 제공
      (partial obs와 sharing obs의 차이점)
 - 공통 부분을 base_runner로 빼낸 뒤, 이를 상속해서 각각의 환경에서 가져다 씀
 - runner 환경이 바뀌면, 새로운 state, aciton space에 대한 정의가 필요해 보인다.
       예를 들어서, multiple BS 환경에서는 이를 Orchestration해주는 RIC 서버를 둬야할 것 같다. 
       이 과정에서 추가적으로 아이디어를 가져갈 수 있어 보인다.

on-policy/scripts
 -> main 코드가 여기에 들어있음
 - train/train_mpe.py를 통해서 main함수 진입
 - 

on-policy/onpolicy/config.py
 - config.py에서 시뮬레이션 설정을 다 세팅한다. argparse API를 사용해서 parameter 관리를 한다.
 - 현재 내 코드에는 main.py에 있는데 떼낼 예정이다. 


* Agent 선언은 어디서? -> envs의 core.py에서 구성한다.
* 여기서 User에 대한 선언도 해줘야 한다.
* core의 World class를 통해서 step, integrate_state, update_agent_state 등의 함수 구현 

warmup에서 대략의 init 과정을 수행해줘야 한다