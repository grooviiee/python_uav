<MAPPO> - 2022년에 나온 "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games" 논문 검증을 위한 시뮬레이터
타 알고리즘과 동일하게 CTDE 방식을 사용해서 학습한다.

* 이 repo는 env가 여러개라서 각자 env마다 돌아가는 코드가 다른다.
* 여기서는 SMAC 시뮬레이터 기준으로 설명 ("smac_runner.py"파일에서 실행)

기존과 동일하게 일정 Step동안 buffer에 저장 후, update 진행

on-policy/algorithms
 -> 실질적으로 RL Training을 실행하는 폴더
 - 그런데 알고리즘에 폴더가 꽤 많다.
 -- r_mappo/r_actor_critic.py : 
    - Actor와 critic 클래스가 존재.
	-- Actor: Outputs actions given observations.
	-- Critic: Outputs value function predictions given centralized input (MAPPO) or local observations (IPPO).
 -- r_mappo/rMAPPOPolicy.py : Wraps actor and critic networks to compute actions and value function predictions.
 - __init__.py : 
 - r_mappo.py : Trainer class for MAPPO to update policies.
 
 

on-policy/envs
 -> 여러가지 환경에서의 강화학습을 제공한다. 여기는 그 환경에 대한 코드이다.
 - envs/mpe를 주로 보면 될 듯 하다.
 - core.py 부분에서 agent와 user의 행동에 따른 state를 수정한다.
  
 
on-policy/runner
 - seperated와 shared 환경을 제공한다.
 - 공통 부분을 base_runner로 빼낸 뒤, 이를 상속해서 각각의 환경에서 가져다 쓴다.
 - 생각을 해보니, multi mbs 환경은 상속이 어려울 수도 있을 것
같다. ric 서버를 둬야할 것 같다는 생각이 든다. 

on-policy/scripts
 -> main 코드가 여기에 들어있음
 - train/train_mpe.py를 통해서 main함수 진입
 - 

on-policy/onpolicy/config.py
 - config.py에서 시뮬레이션 설정을 다 세팅한다. argparse API를 사용해서 parameter 관리를 한다.
 - 현재 내 코드에는 main.py에 있는데 떼낼 예정이다. 


* Agent 선언은 어디서? -> envs의 core.py에서 구성한다.
* 여기서 User에 대한 선언도 해줘야 한다.
* core의 World class를 통해서 step, integrate_state, update_agent_state 등의 함수 구현 

warmup에서 대략의 init 과정을 수행해줘야 한다